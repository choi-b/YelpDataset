# -*- coding: utf-8 -*-
"""LSTM_pt2 with Visualizations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jv4V-YpGKMooHQ1u9Xyh-7EPVvpQ_58d
"""

# Mount Google Drive (required if running on Google Colab)
from google.colab import drive
drive.mount('/content/drive')

# Install required modules
# !pip install '/content/drive/Team Drives/Deep Learning Team Drive/yelp_dataset_challenge-master/yelp_util'
!pip install Cython
# !pip install word2vec
!pip install gensim
!pip install unidecode
!pip install textblob
!pip install wordcloud


# Import required modules
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# import yelp_util
# from yelp_util import downloader as dl
import os
from scipy import misc
import numpy as np
import tensorflow as tf
from gensim.models import Word2Vec
import pandas as pd
import random
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Download and unpickle data
download_path =  '/content/drive/Team Drives/Deep Learning Team Drive/data'
review = pd.read_pickle(os.path.join(download_path, 'chinese_reviews.pickle'))
#review = review.loc[:, ['stars','text']] #use only stars & text columns

#PREOPROCESS A LITTLE BIT

review2 = review.loc[:, ['stars','text']]
#review2.head()
review3 = review.loc[:, ['stars','text']]

#Option 1: review2 makes stars > 3 = positive, and the rest negative

review2['sentiment']=['pos' if (x>3) else 'neg' for x in review2['stars']]
#review2.head()

#Option 2: review3 makes stars > 3 = positive, and stars = 2 or 1 negative. 3 are neutral/not considered for analysis.

#get rid of rows with 3 stars
review3 = review3[review3.stars != 3]
review3.stars.value_counts() # check that there are no "3" stars
review3['sentiment']=['pos' if (x>3) else 'neg' for x in review3['stars']] #add the sentiment column
review3.head()

#lowercase/remove punctuations

import re
#review2['text']= [x.lower() for x in review2['text']]
#review2['text'] = review2['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
#review2.head()

review3['text']= [x.lower() for x in review3['text']]
review3['text'] = review3['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
review3.head()

#review2.dtypes
#review3.dtypes

##Visualizations

## WordCloud for reviews with 1 or 2 stars ("negative" reviews) 
neg_review = review3[(review3['stars']==1) | (review3['stars']==2)]
neg_string = []
for t in neg_review.text:
    neg_string.append(t)
neg_string = pd.Series(neg_string).str.cat(sep=' ')

#negative reviews WordCloud (1,2 stars)
wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

pos_review = review3[(review3['stars']==4) | (review3['stars']==5)]
pos_string = []
for t in pos_review.text:
    pos_string.append(t)
pos_string = pd.Series(pos_string).str.cat(sep=' ')

#Positive reviews WordCloud (4,5 stars)
wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#################
#### L S T M ####
##### starts ####
##### here ###### 
#################

from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, CuDNNLSTM, Bidirectional
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical

tokenizer = Tokenizer(num_words=2000, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',                                  
                      lower=True,split=' ')
tokenizer.fit_on_texts(review2['text'].values)
#tokenizer.fit_on_texts(review3['text'].values)

#Use Tokenizer to vectorize the text and convert it into sequence of integers 
tokenizer

#X = tokenizer.texts_to_sequences(review2['text'].values)
X = tokenizer.texts_to_sequences(review3['text'].values)
X = pad_sequences(X)

#len(review2) #output: 51984
len(review3) #output: 43017

### The following sections ###
### were exectued with the following hyper parameters ###
#embed_dim = 128
#lstm_out = 200
#batch_size = 32
#dropout layer
#vocabulary size = 2000

#activation function - softmax
#optimized = sgd

embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(lstm_out, dropout = 0.2, recurrent_dropout = 0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='sgd',metrics = ['accuracy'])
print(model.summary())

#Y = pd.get_dummies(review2['sentiment']).values
Y = pd.get_dummies(review3['sentiment']).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

#run with "review3" - option 2 with SGD optimized
#takes about 55 minutes to run each epoch...
model.fit(X_train, Y_train, batch_size = batch_size, epochs = 1, verbose = 2)

#score: evaluation of the loss function
#loss/accuracy
score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

#run with "review3" - option 2 with Adams optimizer accuracy is higher! than option 1
model.fit(X_train, Y_train, batch_size = batch_size, epochs = 1, verbose = 2)

#score: evaluation of the loss function
#loss/accuracyscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

### The following sections ###
### were exectued with the following hyper parameters ###
#embed_dim = 128
#lstm_out = 200
#batch_size = 32
#dropout layer
#vocabulary size = 2000

#activation function - softmax
#optimized = adam
#uses CuDNNLSTM !!!!! (GPU powered)


embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(CuDNNLSTM(lstm_out, return_sequences=False, input_shape=(864, 128)))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

#Y = pd.get_dummies(review2['sentiment']).values
Y = pd.get_dummies(review3['sentiment']).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

#run with "review3" - option 2 with ADAM optimizer
#sooooooooo much faster!!!
model.fit(X_train, Y_train, batch_size = batch_size, epochs = 3, verbose = 2)

#score: evaluation of the loss function
#loss/accuracy
score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

#LET'S PLOT IT
history = model.fit(X, Y, validation_split=0.1, epochs=10, batch_size=batch_size, verbose=2)

print(history.history.keys())

# plot accuracy over 10 epochs
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# plot loss over 10 epochs
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

###################
## Bidirectional ##
##### L S T M #####
###### starts #####
####### here ######
###################

from keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D

embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(10, return_sequences=True),
                        input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(2,activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics = ['accuracy'])

print(model.summary())