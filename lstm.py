# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gcKg5N3GCaEFESXt2cxrEYnD1euDTWwz
"""

# Mount Google Drive (required if running on Google Colab)
from google.colab import drive
drive.mount('/content/drive')

# Install required modules
# !pip install '/content/drive/Team Drives/Deep Learning Team Drive/yelp_dataset_challenge-master/yelp_util'
!pip install Cython
# !pip install word2vec
!pip install gensim
!pip install unidecode
!pip install textblob


# Import required modules
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# import yelp_util
# from yelp_util import downloader as dl
import os
from scipy import misc
import numpy as np
import tensorflow as tf
from gensim.models import Word2Vec
import pandas as pd
import random

# Download and unpickle data
download_path =  '/content/drive/Team Drives/Deep Learning Team Drive/data'
review = pd.read_pickle(os.path.join(download_path, 'chinese_reviews.pickle'))
#review = review.loc[:, ['stars','text']] #use only stars & text columns

## <Preprocesing Tips> ##

## CLEANING DATA ##
#import string as str

#all lowercase
#review.text = review.text.apply(lambda x: x.lower())

#remove punctuations
#review.text = review.text.str.replace('[^\w\s]','')

#remove stop words
#from nltk.corpus import stopwords
#stop = stopwords.words('english')
#review.text = review.text.apply(lambda x: " ".join(x for x in x.split() if x not in stop))

#review[:10]

#10 most common words.
#freq = pd.Series(' '.join(review.text).split()).value_counts()[:10]
#freq

#remove common words
#remove: Chinese, food, place, one
#freq = ['food','chinese','place','one']
#review.text = review.text.apply(lambda x: " ".join(x for x in x.split() if x not in freq))
#review[:10] #notice the word chinese is gone in row 9

#10 most rare words
#freq = pd.Series(' '.join(review.text).split()).value_counts()[-10:]
#freq
#remove those 10 words (as an example) - may be ok with removing more..
#freq = list(freq.index)
#review.text = review.text.apply(lambda x: " ".join(x for x in x.split() if x not in freq))
#review[:10]

#spelling correction
#from textblob import TextBlob
#review.text[:5].apply(lambda x: str(TextBlob(x).correct()))

#################
#### L S T M ####
##### starts ####
##### here ######
#################


#Try another way to build LSTM (using the whole dataset)

review2 = review.loc[:, ['stars','text']]
#review2.head()
#review3 = review.loc[:, ['stars','text']]

#Option 1: review2 makes stars > 3 = positive, and the rest negative

review2['sentiment']=['pos' if (x>3) else 'neg' for x in review2['stars']]
review2.head()

#Option 2: review3 makes stars > 3 = positive, and stars = 2 or 1 negative. 3 are neutral/not considered for analysis.

#get rid of rows with 3 stars
#review3 = review3[review3.stars != 3]
#review3.stars.value_counts() # check that there are no "3" stars
#review3['sentiment']=['pos' if (x>3) else 'neg' for x in review3['stars']] #add the sentiment column
#review3.head()

#lowercase/remove punctuations

import re
review2['text']= [x.lower() for x in review2['text']]
review2['text'] = review2['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
review2.head()

#review3['text']= [x.lower() for x in review3['text']]
#review3['text'] = review3['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
#review3.head()

#review2.dtypes

from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical

tokenizer = Tokenizer(num_words=2000, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',                                  
                      lower=True,split=' ')
tokenizer.fit_on_texts(review2['text'].values)
#tokenizer.fit_on_texts(review3['text'].values)

#Use Tokenizer to vectorize the text and convert it into sequence of integers 
tokenizer

X = tokenizer.texts_to_sequences(review2['text'].values)
#X = tokenizer.texts_to_sequences(review3['text'].values)
X = pad_sequences(X)

len(review2) #output: 51984
#len(review3) #output: 43017

### The following sections ###
### were exectued with the following hyper parameters ###
#embed_dim = 128
#lstm_out = 196
#batch_size = 32
#dropout layer
#vocabulary size = 2000

#activation function - softmax

embed_dim = 128
lstm_out = 196
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(lstm_out, dropout = 0.2, recurrent_dropout = 0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

#run with "review2" - option 1
#takes about 55 minutes to run each epoch...
model.fit(X_train, Y_train, batch_size = batch_size, epochs = 1, verbose = 2)

#seems like we definitely need to use more epochs to increase accuracy (for option 1 esp.).
score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

#Varying test sizes
#let's try it with a test_size = 0.3
Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X,Y, test_size = 0.3, random_state = 42)
print(X_train2.shape,Y_train2.shape)
print(X_test2.shape,Y_test2.shape)

#run with "review2" - option 1
#test size = 0.3
model.fit(X_train2, Y_train2, batch_size = batch_size, epochs = 1, verbose = 2)

#loss and test acc. with test size = 0.3
score,acc = model.evaluate(X_test2, Y_test2, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

#let's try it with a test_size = 0.1
Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train3, X_test3, Y_train3, Y_test3 = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train3.shape,Y_train3.shape)
print(X_test3.shape,Y_test3.shape)

#run with "review2" - option 1
#test size = 0.1
model.fit(X_train3, Y_train3, batch_size = batch_size, epochs = 1, verbose = 2)

#SoftmaxZeroPointOne
#loss and test acc. with test size = 0.1
score,acc = model.evaluate(X_test3, Y_test3, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

#run with "review3" - option 2 - accuracy is higher! than option 1
#takes about 45 minutes to run
model.fit(X_train, Y_train, batch_size = batch_size, epochs = 1, verbose = 2)

#This was for "review3" - option 2
score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

### The following sections ###
### were exectued with the following hyper parameters ###
#embed_dim = 128
#lstm_out = 100 *instead of 196
#batch_size = 64 *instead of 32

#no dropout layer (dropout layer helps with overfitting)
#vocabulary size = 1000

embed_dim = 128
lstm_out = 100
batch_size = 64

model = Sequential()
model.add(Embedding(1000, embed_dim,input_length = X.shape[1]))
model.add(LSTM(lstm_out))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train4, X_test4, Y_train4, Y_test4 = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train4.shape,Y_train4.shape)
print(X_test4.shape,Y_test4.shape)

#run with "review2" - option 1
#test size = 0.1
model.fit(X_train4, Y_train4, batch_size = batch_size, epochs = 1, verbose = 2)

#This was for "review2" - option 1
score,acc = model.evaluate(X_test4, Y_test4, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

#training loss << test loss = may be some overfitting here due to the missing dropout layer.

### The following sections ###
### were exectued with the following hyper parameters ###
#embed_dim = 128
#lstm_out = 200
#batch_size = 32
#dropout layer
#vocabulary size = 2000

#"tanh" activation function

embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(lstm_out, dropout = 0.2, recurrent_dropout = 0.2))
model.add(Dense(2,activation='tanh'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train5, X_test5, Y_train5, Y_test5 = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train5.shape,Y_train5.shape)
print(X_test5.shape,Y_test5.shape)

#run with "review2" - option 1
#test size = 0.1
#tanh activation
model.fit(X_train5, Y_train5, batch_size = batch_size, epochs = 1, verbose = 2)

#This was for "review2" - option 1
#"tanh" activation function
score,acc = model.evaluate(X_test5, Y_test5, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

### The following sections ###
### were exectued with the following hyper parameters ###
#embed_dim = 128
#lstm_out = 200
#batch_size = 32
#dropout layer
#vocabulary size = 2000

#"sigmoid" activation function

embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(lstm_out, dropout = 0.2, recurrent_dropout = 0.2))
model.add(Dense(2,activation='sigmoid'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train6, X_test6, Y_train6, Y_test6 = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train5.shape,Y_train6.shape)
print(X_test5.shape,Y_test6.shape)

#run with "review2" - option 1
#test size = 0.1
#sigmoid activation - very similar to Softmax, acc is a little bit lower than softmax.
model.fit(X_train6, Y_train6, batch_size = batch_size, epochs = 1, verbose = 2)

#This was for "review2" - option 1
#"sigmoid" activation function
score,acc = model.evaluate(X_test6, Y_test6, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))

########################
#to compare this with Softmax
#look up (control + F)
#SoftmaxZeroPointOne
#vs
#SigmoidZeroPointOne

###################
## Bidirectional ##
##### L S T M #####
###### starts #####
####### here ######
###################

from keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D

embed_dim = 128
lstm_out = 200
batch_size = 32

model = Sequential()
model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(10, return_sequences=True),
                        input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(2,activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics = ['accuracy'])

print(model.summary())

Y = pd.get_dummies(review2['sentiment']).values
#Y = pd.get_dummies(review3['sentiment']).values
X_train7, X_test7, Y_train7, Y_test7 = train_test_split(X,Y, test_size = 0.1, random_state = 42)
print(X_train7.shape,Y_train7.shape)
print(X_test7.shape,Y_test7.shape)

model.fit(X_train7, Y_train7, batch_size = batch_size, epochs = 1, verbose = 2)

score,acc = model.evaluate(X_test7, Y_test7, verbose = 2, batch_size = batch_size)
print("Score: %.2f" % (score))
print("Validation Accuracy: %.2f" % (acc))